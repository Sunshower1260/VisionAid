import { CameraView, useCameraPermissions } from "expo-camera";
import * as Speech from "expo-speech";
import React, { useRef, useState } from "react";
import {
  ActivityIndicator,
  Alert,
  Image,
  StyleSheet,
  Text,
  TouchableOpacity,
  View,
} from "react-native";

export default function CameraScreen() {
  const [permission, requestPermission] = useCameraPermissions();
  const cameraRef = useRef<any>(null);
  const [photo, setPhoto] = useState<string | null>(null);
  const [loading, setLoading] = useState(false);
  const [isResultShown, setIsResultShown] = useState(false); 

  if (!permission) return <View />;
  if (!permission.granted) {
    return (
      <View style={styles.container}>
        <Text>B·∫°n c·∫ßn c·∫•p quy·ªÅn camera ƒë·ªÉ s·ª≠ d·ª•ng</Text>
        <TouchableOpacity style={styles.button} onPress={requestPermission}>
          <Text style={styles.text}>C·∫•p quy·ªÅn</Text>
        </TouchableOpacity>
      </View>
    );
  }

  const takePhoto = async () => {
    if (isResultShown) {
      setPhoto(null);
      setIsResultShown(false);
      return;
    }

    if (!cameraRef.current) return;

    setLoading(true);
    try {
      console.log("üì∏ B·∫Øt ƒë·∫ßu ch·ª•p ·∫£nh...");
      const photoData = await cameraRef.current.takePictureAsync({ base64: true });
      console.log("‚úÖ ·∫¢nh ƒë√£ ch·ª•p:", photoData.uri);
      setPhoto(photoData.uri);

      console.log("üì§ G·ª≠i ·∫£nh l√™n server...");
      const res = await fetch("https://visionaid-be.onrender.com/analyze", {
        method: "POST",
        headers: { "Content-Type": "application/json" },
        body: JSON.stringify({ image: photoData.base64 }),
      });

      console.log("üì• Nh·∫≠n ph·∫£n h·ªìi th√¥...");
      const textResponse = await res.text();
      console.log("üßæ Raw response:", textResponse);

      let data;
      try {
        data = JSON.parse(textResponse);
      } catch (parseErr) {
        console.error("‚ùå L·ªói parse JSON:", parseErr);
        Alert.alert("L·ªói ph·∫£n h·ªìi", "Server tr·∫£ v·ªÅ d·ªØ li·ªáu kh√¥ng h·ª£p l·ªá");
        return;
      }

      console.log("‚úÖ JSON h·ª£p l·ªá:", data);

      if (data.success && data.text) {
        let contentToSpeak = data.text.trim();

        const match = data.text.match(/N·ªôi dung[:Ôºö]\s*(.*)/s);
        if (match && match[1]) contentToSpeak = match[1].trim();

        if (/^Th·ªÉ lo·∫°i:/i.test(contentToSpeak) && !match) {
          console.warn("‚è≠Ô∏è Kh√¥ng c√≥ n·ªôi dung th·ª±c t·∫ø ƒë·ªÉ ƒë·ªçc.");
          setIsResultShown(true);
          return;
        }

        console.log("üîä G·ªçi Speech.speak v·ªõi n·ªôi dung:", contentToSpeak);
        Speech.stop();

        Speech.speak(contentToSpeak, {
          language: "vi-VN",
          rate: 1.0,
          pitch: 1.0,
          onStart: () => console.log("üéôÔ∏è B·∫Øt ƒë·∫ßu ƒë·ªçc..."),
          onDone: () => {
            console.log("‚úÖ ƒê·ªçc xong!");
            setIsResultShown(true); // ‚úÖ Cho ph√©p b·∫•m ‚ÄúCh·ª•p l·∫°i‚Äù
          },
          onError: (e) => {
            console.error("‚ùå L·ªói khi ƒë·ªçc:", e);
            setIsResultShown(true);
          },
        });
      } else {
        console.warn("‚ö†Ô∏è Server kh√¥ng tr·∫£ v·ªÅ text:", data);
        Alert.alert("Ph√¢n t√≠ch th·∫•t b·∫°i", data.error || "Kh√¥ng nh·∫≠n ƒë∆∞·ª£c n·ªôi dung t·ª´ server");
        setIsResultShown(true);
      }
    } catch (err: any) {
      console.error("‚ùå L·ªói khi g·ª≠i ·∫£nh:", err);
      Alert.alert("L·ªói", err.message || "Kh√¥ng th·ªÉ g·ª≠i ·∫£nh ƒë·∫øn server");
      setIsResultShown(true);
    } finally {
      setLoading(false);
      console.log("üì∑ Ho√†n t·∫•t x·ª≠ l√Ω.");
    }
  };

  return (
    <View style={{ flex: 1 }}>
      {photo && isResultShown ? (
        <Image source={{ uri: photo }} style={{ flex: 1 }} />
      ) : (
        <CameraView ref={cameraRef} style={{ flex: 1 }} facing="back" />
      )}

      {loading && (
        <View style={styles.loadingOverlay}>
          <ActivityIndicator size="large" color="#fff" />
          <Text style={{ color: "#fff", marginTop: 8 }}>ƒêang x·ª≠ l√Ω...</Text>
        </View>
      )}

      <TouchableOpacity
        style={[styles.button, loading && { opacity: 0.5 }]}
        onPress={takePhoto}
        disabled={loading}
      >
        <Text style={styles.text}>
          {isResultShown ? "üì∏ Ch·ª•p l·∫°i & Ph√¢n t√≠ch" : "üì∏ Ch·ª•p & Ph√¢n t√≠ch"}
        </Text>
      </TouchableOpacity>
    </View>
  );
}

const styles = StyleSheet.create({
  container: { flex: 1, justifyContent: "center", alignItems: "center" },
  button: {
    position: "absolute",
    bottom: 40,
    alignSelf: "center",
    backgroundColor: "#2563EB",
    padding: 14,
    borderRadius: 8,
  },
  text: { color: "#fff", fontWeight: "bold", fontSize: 16 },
  loadingOverlay: {
    position: "absolute",
    top: 0,
    left: 0,
    right: 0,
    bottom: 0,
    justifyContent: "center",
    alignItems: "center",
    backgroundColor: "rgba(0,0,0,0.5)",
  },
});
